{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGCNN experiments as embedding method (LION15 SI)\n",
    "In this notebook we implement a Deep Graph Convolutional Neural Network (DGCNN) [1] algorithm according to the colab: [Supervised graph classification with Deep Graph CNN\n",
    "](https://colab.research.google.com/github/stellargraph/stellargraph/blob/master/demos/graph-classification/dgcnn-graph-classification.ipynb).\n",
    "\n",
    "The DGCNN implememntation uses the [StellarGraph](https://www.stellargraph.io/) Library. The DGCNN implementation has been modified in such a way to function as an inductive graph embedding method. To this aim the DGCNN model is fed with input graphs and the output of the last hidden layer is extracted and used as the embedding of the input graph. The graph embedding are then used as input to a SVM with linear kernel for 10-fold cross-validation.\n",
    "\n",
    "In addition, our modified version of the DGCNN accepts has input graphml files by means of the [NetworkX](https://networkx.org/) Library.\n",
    "\n",
    "**References**\n",
    "\n",
    "[1] An End-to-End Deep Learning Architecture for Graph Classification, M. Zhang, Z. Cui, M. Neumann, Y. Chen, AAAI-18. ([link](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17146))\n",
    "\n",
    "[2] Semi-supervised Classification with Graph Convolutional Networks, T. N. Kipf and M. Welling, ICLR 2017. ([link](https://arxiv.org/abs/1609.02907))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Stellargraph\n",
    "The folowing code installs the StellarGraph library and all the other python packages required for execution (TensorFlow, NetworkX, Pandas, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# install StellarGraph if running on Google Colab\n",
    "!pip install -g stellargraph\n",
    "!pip install -q sklean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from time import time\n",
    "import stellargraph as sg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN, GCNSupervisedGraphClassification\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph import datasets\n",
    "from stellargraph.datasets.dataset_loader import DatasetLoader\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,matthews_corrcoef,accuracy_score,precision_score,f1_score, recall_score\n",
    "import tqdm as tq\n",
    "import networkx as nx\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loading function\n",
    "Te following code loads a dataset in TID format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, root_dir='.', edge_labels_as_weights=False):\n",
    "\n",
    "    def _load_from_txt_file(filename, names=None, dtype=None, index_increment=None):\n",
    "        df = pd.read_csv(\n",
    "            f\"{root_dir}/{dataset}/{dataset}_{filename}.txt\",\n",
    "            header=None,\n",
    "            index_col=False,\n",
    "            dtype=dtype,\n",
    "            names=names,\n",
    "        )\n",
    "        # We optional increment the index by 1 because indexing, e.g. node IDs, for this dataset starts\n",
    "        # at 1 whereas the Pandas DataFrame implicit index starts at 0 potentially causing confusion selecting\n",
    "        # rows later on.\n",
    "        if index_increment:\n",
    "            df.index = df.index + index_increment\n",
    "        return df\n",
    "\n",
    "    # edge information:\n",
    "    df_graph = _load_from_txt_file(filename=\"A\", names=[\"source\", \"target\"])\n",
    "\n",
    "    if edge_labels_as_weights and os.path.exists(f\"{root_dir}/{dataset}/{dataset}_node_attributes.txt\"):\n",
    "        # there's some edge labels, that can be used as edge weights\n",
    "        df_edge_labels = _load_from_txt_file(\n",
    "            filename=\"edge_labels\", names=[\"weight\"], dtype=int\n",
    "        )\n",
    "        df_graph = pd.concat([df_graph, df_edge_labels], axis=1)\n",
    "\n",
    "    # node information:\n",
    "    df_graph_ids = _load_from_txt_file(\n",
    "        filename=\"graph_indicator\", names=[\"graph_id\"], index_increment=1\n",
    "    )\n",
    "\n",
    "    df_node_labels = _load_from_txt_file(\n",
    "        filename=\"node_labels\", dtype=\"category\", index_increment=1\n",
    "    )\n",
    "    # One-hot encode the node labels because these are used as node features in graph classification\n",
    "    # tasks.\n",
    "    df_node_features = pd.get_dummies(df_node_labels)\n",
    "\n",
    "    if os.path.exists(f\"{root_dir}/{dataset}/{dataset}_node_attributes.txt\"):\n",
    "        # there's some actual node attributes\n",
    "        df_node_attributes = _load_from_txt_file(\n",
    "            filename=\"node_attributes\", dtype=np.float32, index_increment=1\n",
    "        )\n",
    "\n",
    "        df_node_features = pd.concat([df_node_features, df_node_attributes], axis=1)\n",
    "\n",
    "    # graph information:\n",
    "    df_graph_labels = _load_from_txt_file(\n",
    "        filename=\"graph_labels\", dtype=\"category\", names=[\"label\"], index_increment=1\n",
    "    )\n",
    "\n",
    "    # split the data into each of the graphs, based on the nodes in each one\n",
    "    def graph_for_nodes(nodes):\n",
    "        # each graph is disconnected, so the source is enough to identify the graph for an edge\n",
    "        edges = df_graph[df_graph[\"source\"].isin(nodes.index)]\n",
    "        return StellarGraph(nodes, edges)\n",
    "\n",
    "    groups = df_node_features.groupby(df_graph_ids[\"graph_id\"])\n",
    "    graphs = [graph_for_nodes(nodes) for _, nodes in groups]\n",
    "\n",
    "    return graphs, df_graph_labels[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The edge removal function\n",
    "THis function implements the graph attack strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_edgeattack(G: nx.Graph, criteria = \"random\", percentage=30, verbose=False, random_state=42):\n",
    "  at = percentage/100.0\n",
    "  #remove_zero_weights(G)\n",
    "  if criteria == \"betweeness\":\n",
    "    score = nx.edge_betweenness(G).items()\n",
    "  elif criteria == \"degree\":\n",
    "    raise Exception(\"Wrong criteria\")\n",
    "  elif criteria == \"random\":\n",
    "    score = list(G.edges())\n",
    "    random.Random(random_state).shuffle(score)\n",
    "    score = list(dict(zip(score,range(len(score)))).items())\n",
    "  else:\n",
    "    raise Exception(\"Wrong criteria\")\n",
    "  edges_to_remove = sorted(score, key=operator.itemgetter(1, 0), reverse=True)[0:int(len(score)*at)]\n",
    "  #assert len(edges_to_remove) > 0, \"Nothing to remove!\"\n",
    "  for e,w in edges_to_remove:\n",
    "    G.remove_edge(e[0], e[1])\n",
    "  if verbose:\n",
    "    print(\"removed\", edges_to_remove)\n",
    "  return 0,len(edges_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset (and attack a copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copy/Attack: 100%|██████████| 188/188 [00:01<00:00, 152.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 17, Edges: 38\n",
      "\n",
      " Node types:\n",
      "  default: [17]\n",
      "    Features: float32 vector, length 7\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [38]\n",
      "        Weights: all 1 (default)\n",
      "        Features: none\n",
      "     1\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "5    0\n",
      "..  ..\n",
      "184  1\n",
      "185  0\n",
      "186  0\n",
      "187  1\n",
      "188  0\n",
      "\n",
      "[188 rows x 1 columns]\n",
      "No Classes 1\n",
      "\n",
      "       (Graph) nodes  (Graph) edges  (Attacked Graph) nodes  \\\n",
      "count         188.00         188.00                  188.00   \n",
      "mean           17.93          39.59                   17.93   \n",
      "std             4.59          11.40                    4.59   \n",
      "min            10.00          20.00                   10.00   \n",
      "25%            14.00          28.00                   14.00   \n",
      "50%            17.50          38.00                   17.50   \n",
      "75%            22.00          50.00                   22.00   \n",
      "max            28.00          66.00                   28.00   \n",
      "\n",
      "       (Attacked Graph) edges  \n",
      "count                  188.00  \n",
      "mean                    29.99  \n",
      "std                      8.56  \n",
      "min                     15.00  \n",
      "25%                     21.00  \n",
      "50%                     29.00  \n",
      "75%                     38.00  \n",
      "max                     50.00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm as tq\n",
    "dataname = 'MUTAG' #@param ['MUTAG', 'KIDNEY', 'Kidney_9.2', 'PROTEINS', 'JE']\n",
    "criteria = 'random' #@param ['random', 'betweeness']\n",
    "percentage = 50 #@param [0, 5, 10, 20, 30, 40, 50] {type:\"raw\"}\n",
    "ontest = False #@param {type:\"boolean\"}\n",
    "import shutil\n",
    "shutil.unpack_archive(f'../datasets/{dataname}.zip', '../datasets/')\n",
    "graphs, graph_labels = load_dataset(dataname, root_dir='../datasets')\n",
    "graphsadv = []  # no attack if dataset is loaded from TU format\n",
    "for G in tq.tqdm(graphs, desc=\"Copy/Attack\"):\n",
    "  Gadv = G.to_networkx()\n",
    "  ec = Gadv.number_of_edges()\n",
    "  if percentage > 0: \n",
    "    n,e = nx_edgeattack(Gadv, criteria=criteria, percentage=percentage, random_state=42)\n",
    "  graphsadv += [sg.StellarGraph.from_networkx(Gadv,node_type_default=\"default\", edge_type_default=\"default\", node_type_attr=\"type\", edge_type_attr=\"type\", edge_weight_attr=\"weight\", node_features='feature')]\n",
    "if graph_labels.nunique() > 2:\n",
    "  y = pd.get_dummies(graph_labels)\n",
    "else:\n",
    "  y = pd.get_dummies(graph_labels, drop_first=True)\n",
    "print(graphs[0].node_features())\n",
    "print(graphs[0].info())\n",
    "print(y)\n",
    "nclasses = len(y.columns)\n",
    "print(\"No Classes %d\\n\"%nclasses)\n",
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges(),ga.number_of_nodes(), ga.number_of_edges()) for g,ga in zip(graphs,graphsadv)],\n",
    "    columns=[\"(Graph) nodes\", \"(Graph) edges\", \"(Attacked Graph) nodes\", \"(Attacked Graph) edges\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print graphs statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       (Graph) nodes  (Graph) edges  (Attacked Graph) nodes  \\\n",
      "count         188.00         188.00                  188.00   \n",
      "mean           17.93          39.59                   17.93   \n",
      "std             4.59          11.40                    4.59   \n",
      "min            10.00          20.00                   10.00   \n",
      "25%            14.00          28.00                   14.00   \n",
      "50%            17.50          38.00                   17.50   \n",
      "75%            22.00          50.00                   22.00   \n",
      "max            28.00          66.00                   28.00   \n",
      "\n",
      "       (Attacked Graph) edges  \n",
      "count                  188.00  \n",
      "mean                    29.99  \n",
      "std                      8.56  \n",
      "min                     15.00  \n",
      "25%                     21.00  \n",
      "50%                     29.00  \n",
      "75%                     38.00  \n",
      "max                     50.00  \n"
     ]
    }
   ],
   "source": [
    "print(summary.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GCNN model\n",
    "def create_dgcnn_model(generator, size, nouts, learnrate=0.0001):\n",
    "      k = 35  # the number of rows for the output tensor\n",
    "      layer_sizes = [size, size, size, nouts]\n",
    "      dgcnn_model = DeepGraphCNN(layer_sizes=layer_sizes,activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],k=k,bias=False,generator=generator)\n",
    "      x_inp, x_out = dgcnn_model.in_out_tensors()\n",
    "      x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "      x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "      x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "      x_out = Flatten()(x_out)\n",
    "      x_out = Dense(units=size, activation=\"relu\")(x_out)\n",
    "      x_out = embedlayer = Dropout(rate=0.5)(x_out)\n",
    "      if nouts > 2:\n",
    "        predictions = Dense(units=nouts, activation=\"softmax\")(x_out)\n",
    "        model = Model(inputs=x_inp, outputs=predictions)\n",
    "        model.compile(optimizer=Adam(lr=learnrate), loss=categorical_crossentropy, metrics=[\"acc\"],)\n",
    "      else:\n",
    "        predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "        model = Model(inputs=x_inp, outputs=predictions)\n",
    "        model.compile(optimizer=Adam(lr=learnrate), loss=binary_crossentropy, metrics=[\"acc\"],)\n",
    "      embedding = Model(inputs=x_inp, outputs=embedlayer)\n",
    "      return model, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Experimental Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold: 100%|██████████| 10/10 [04:55<00:00, 29.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 44  19]\n",
      " [ 11 114]]\n",
      "Acc\t84.09±6.13\n",
      "Prec\t0.85±0.08\n",
      "F1\t0.81±0.08\n",
      "Recall\t0.81±0.09\n",
      "MCC\t0.65±0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters\n",
    "params = {\"layerdim\": 32, \"epochs\": 100, \"learningrate\": 0.0001, \"verbose\": False, \"seed\": 42}\n",
    "\n",
    "# Validation\n",
    "start = time()\n",
    "tot_preds = np.array([])\n",
    "tot_targets = np.array([])\n",
    "tot_acc = np.array([])\n",
    "tot_prec = np.array([])\n",
    "tot_F1 = np.array([])\n",
    "tot_recall = np.array([])\n",
    "tot_MCC = np.array([])\n",
    "cv_folds = 10\n",
    "tsize = 1.0 - (1.0 / float(cv_folds))\n",
    "test_metrics = []\n",
    "verbose = 1 if params['verbose'] else 0\n",
    "cv_folds = 10 #@param {type:\"slider\", min:2, max:10, step:1}\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "setlist = []\n",
    "if nclasses > 2:\n",
    "  yy = np.argmax(y.values, axis=1)\n",
    "  y = y.to_numpy()\n",
    "else:\n",
    "  y = y.to_numpy()\n",
    "  yy = y\n",
    "for train_index, test_index in tq.tqdm(list(skf.split(graphs,yy)), desc=\"fold: \"):\n",
    "  train_graphs = graph_labels[train_index+1]\n",
    "  test_graphs = graph_labels[test_index+1]\n",
    "  setlist += [set(test_index)]\n",
    "  gen = PaddedGraphGenerator(graphs=graphs)\n",
    "  genadv = PaddedGraphGenerator(graphs=graphsadv)\n",
    "  train_gen = gen.flow(list(train_graphs.index - 1),targets=y[np.array(train_graphs.index-1)],batch_size=1,symmetric_normalization=False)\n",
    "  test_gen = genadv.flow(list(test_graphs.index - 1),targets=y[np.array(test_graphs.index-1)],batch_size=1,symmetric_normalization=False)\n",
    "  y_train = [x.tolist() for x in y[np.array(train_graphs.index-1)]] \n",
    "  y_test = [x.tolist() for x in y[np.array(test_graphs.index-1)]]\n",
    "  model, embedding = create_dgcnn_model(gen, params['layerdim'], nclasses, learnrate=params['learningrate'])\n",
    "  history = model.fit(train_gen, validation_data=test_gen, shuffle=False, epochs=params['epochs'], verbose=params['verbose'])\n",
    "  X_test = embedding.predict(test_gen)\n",
    "  X_train = embedding.predict(train_gen)\n",
    "  y_pred = SVC(kernel='linear').fit(X_train,y_train).predict(X_test)\n",
    "  tot_preds = np.append(tot_preds,y_pred)\n",
    "  tot_targets = np.append(tot_targets,y_test)\n",
    "  tot_acc = np.append(tot_acc, accuracy_score(y_test, y_pred))\n",
    "  tot_prec = np.append(tot_prec, precision_score(y_test, y_pred, average='macro'))\n",
    "  tot_F1 = np.append(tot_F1, f1_score(y_test, y_pred, average='macro'))\n",
    "  tot_recall = np.append(tot_recall, recall_score(y_test, y_pred, average='macro'))\n",
    "  tot_MCC = np.append(tot_MCC, matthews_corrcoef(y_test, y_pred))\n",
    "temp = time() - start\n",
    "hours = temp//3600\n",
    "temp = temp - 3600*hours\n",
    "minutes = temp//60\n",
    "seconds = temp - 60*minutes\n",
    "expired = '%d:%d:%d' %(hours,minutes,seconds)\n",
    "print()\n",
    "print(confusion_matrix(tot_targets, tot_preds))\n",
    "print(\"Acc\\t%.2f\\u00B1%.2f\"%((tot_acc * 100).mean(), (tot_acc * 100).std()))\n",
    "print(\"Prec\\t%.2f\\u00B1%.2f\"%(tot_prec.mean(), tot_prec.std()))\n",
    "print(\"F1\\t%.2f\\u00B1%.2f\"%(tot_F1.mean(), tot_F1.std()))\n",
    "print(\"Recall\\t%.2f\\u00B1%.2f\"%(tot_recall.mean(), tot_recall.std()))\n",
    "print('MCC\\t%.2f\\u00B1%.2f'%(tot_MCC.mean(), tot_MCC.std()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc6e667b201477635ba32fc377e71e93fe0ce3fc2d2fb508931525558f52d375"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
